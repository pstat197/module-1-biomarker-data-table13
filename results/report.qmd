---
title: "Biomarkers of ASD"
subtitle: "If you want a subtitle put it here"
author: "Lucas Childs, Minu Pabbathi, Nathan Kim, Bahaar Ahuja, Anna Liang"
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

Use this as a template. Keep the headers and remove all other text. In all, your report can be quite short. When it is complete, render and then push changes to your team repository.

```{r}
# load any other packages and read data here
library(tidyverse)
```

## Abstract

Write a brief one-paragraph abstract that describes the contents of your write-up.

This report covers an analysis of blood biomarkers for autism spectrum disorder (ASD), analyzing proteins and their association with ASD. Emulating the paper Hewitson et al. 2021, we analyze the dataset used in the paper, data preprocessing including transformations and outlier trimmings, methodologies for selecting a panel of proteins for ASD classification, and improvement in protein panel selection.

## Dataset

Write a brief data description, including: how data were obtained; sample characteristics; variables measured; and data preprocessing. This can be largely based on the source paper and should not exceed 1-2 paragraphs.

Data was taken via blood serum sample from 76 boys with ASD and 78 typically developing (TD) boys ranging from 18 months - 8 years of age. The serum samples were analyzed to identify possible eraly biological markers for ASD. Proteomic analysis of serum was performed using SomaLogic’s SOMAScan$^{TM}$ assay 1.3K platform. A total of 1,317 proteins were analyzed, with two additional variables `ADOS` and `group` pertaining to ASD severity and whether the subject is TD or has ASD. Data preprocessing was done to eliminate missing values, $log_{10}$ transform biomarker levels to stabilize variance, large values, and normalize skewed distributions, center and scale transformed data to standardize the distribution of each biomarker, and trim outliers to prevent disproportionate influence.

## Summary of published analysis

Summarize the methodology of the paper in 1-3 paragraphs. You need not explain the methods in depth as we did in class; just indicate what methods were used and how they were combined. If possible, include a diagram that depicts the methodological design. (Quarto has support for [GraphViz and Mermaid flowcharts](https://quarto.org/docs/authoring/diagrams.html).) Provide key results: the proteins selected for the classifier and the estimated accuracy.

The original researchers used multiple *t*-tests to see whether there were significant differences in protein levels between the ASD group and the neurotypical group. Based on those *t* values, the top 10 proteins were selected for the prediction model. Each protein was also correlated with ADOS scores and the top 10 highly correlated proteins were selected. The third approach was random forest, which involves using a random forest to predict whether a participant has ASD or is typically developing. By keeping track of which variables were used most to define splits, a variable importance score can be used to determine which predictors were most influential in prediction. Using this method, the top proteins were selected.

After running all three methods and determining the top 10 proteins for each method, 5 proteins that were common to all three methods were selected as the core proteins. Each of the other proteins were added one at a time to see their impact on the AUC, leading to about four additional proteins being classified as optimal proteins.

**Q1:** A logarithmic transformation was done to the biomarker levels in order to reduce scale, variance, and skewedness of the data. Furthermore, the data was centered and scaled to standardize the distribution of each biomarker.

**Q2:** We temporarily removed trimming and flagged protein-level outliers at \$\|z\|\>3\$, then aggregated outliers by subject to identify subject-level outliers. Applying the Tukey boxplot rule to these counts revealed a small set of clear outlier subjects. Descriptively, outlier subjects were more common in TD than ASD, but a two-proportion test indicated the difference was not statistically significant. Overall, extreme values are concentrated in a few participants, so the trimming at \$\\pm3\$ standard deviations mainly reduces their influence rather than indicating a group effect. This validates the preprocessing approach and allows us to proceed with the trimmed data.

**Q3:** To evaluate the sensitivity of the analysis to different methodological choices, we first modified the workflow so that all feature selection was carried out exclusively on a training partition, while a separate test set was held out and used only once at the very end for evaluation. This change avoids data leakage and gives a more realistic estimate of model performance. Next, instead of selecting only the top 10 proteins per method as was done in class, we increased this number to 20, which improved the model’s sensitivity, specificity, and AUC. For example, using 20 proteins, the logistic regression model achieved sensitivity of 0.75, specificity of 0.867, accuracy of 0.806, and ROC AUC of 0.892 on the test set. Finally, we compared a hard intersection of selected proteins to a fuzzy intersection approach. Under the fuzzy intersection, proteins were combined if they appeared in either method, rather than requiring overlap across all methods. This more flexible approach resulted in lower performance (sensitivity = 0.5625, specificity = 0.8000, accuracy = 0.6774, AUC = 0.7833) compared to the hard intersection and full panel, suggesting that although fuzzy selection increases feature stability, it may include weaker predictors and reduce classification accuracy.

**Q4:** We chose to find an alternate protein panel that achieves improved classification accuracy using a guided LASSO approach with differential penalties. Proteins identified through both multiple testing with t-tests and Random Forest importance (`proteins_sstar`) were assigned zero penalty to ensure these were included in the panel without significant shrinkage due to regularization. So, this prior validation was used since there was strong prior evidence that these proteins were relevant. The remaining proteins not selected by the two aforementioned methods received L1 regularization (LASSO regularization), allowing us to discover additional predictive biomarkers while controlling overfitting. We can think of this method from a Bayesian standpoint where the prior data (pre-selected panel of 5 proteins, `proteins_sstar`) informed our belief about protein importance. Overall, our method displayed increased classification accuracy, specifically highlighted by the metric: 'accuracy'. The metrics are depicted below:

$$
\begin{array}{llr}
\hline
\textbf{metric} & \textbf{estimator} & \textbf{estimate} \\
\hline
\text{sensitivity} & \text{binary} & 0.750 \\
\text{specificity} & \text{binary} & 0.933 \\
\text{accuracy} & \text{binary} & 0.839 \\
\text{roc auc} & \text{binary} & 0.858 \\
\hline
\end{array}
$$ Compared to the benchmark results: $$
\begin{array}{llr}
\hline
\textbf{metric} & \textbf{estimator} & \textbf{estimate} \\
\hline
\text{sensitivity} & \text{binary} & 0.812 \\
\text{specificity} & \text{binary} & 0.733 \\
\text{accuracy} & \text{binary} & 0.774 \\
\text{roc auc} & \text{binary} & 0.883 \\
\hline
\end{array}
$$

The proteins selected for the classifier are the following:

-   CD59
-   4-1BB
-   Dtk
-   Cadherin-5
-   HAI-1
-   Kallikrein 11
-   PAI-1
-   Growth hormone receptor
-   IGFBP-4
-   MRC2
-   CRDL1
-   IL-17 RD
-   TPSG1
-   MP2K2
-   ENPP7
-   MFGM
-   PCSK7
-   ITI heavy chain H4
-   IgD
-   DBNL
-   DERM
-   Elafin
-   RELT
-   PPID
-   Semaphorin 3E
-   CD27
-   CNDP1
-   IL-17 RC
-   SRCN1
-   Epo
-   GDNF
-   14-3-3 protein zeta/delta
-   a-Synuclein
-   CSRP3
-   MIG

## Findings

Summarize your findings here. I've included some subheaders in a way that seems natural to me; you can structure this section however you like.

### Impact of preprocessing and outliers

**Task 1:** After looking at the raw protein level distributions for a sample of 4 proteins in the dataset, it became clear that the scale of each protein level was very different across proteins. For example, one protein's mean was 17,164 while another's was 458. Furthermore, we found certain proteins had skewed distributions where one protein had a mean roughly 5,000 units larger than its median, indicating a right skew. Looking at each protein's density further revealed the differing scales of each protein level, large outliers, and skewed distributions apparent in 2 out of the 4 randomly selected proteins.

The log transformation of the protein levels helps to compress the scale of protein levels across the wide range of positive values that were encountered. Additionally, the logarithm helps with skewed data, helping to make data more symmetric and closer to a normal distribution (as confirmed with histograms and a QQ Plot).

**Task 2:** After removing the original trimming, we flagged protein-level outliers at $|z|>3$, where 99.7% of data is captured under a normal curve. Using the Tukey cutoff $(Q3 + 1.5\cdot IQR \approx 29.4)$, the 154 test subjects were considered outliers if they had $\geq 30$ outlier proteins. This flagged 13 outlier subjects, distributed as $\frac{4}{76} \approx 5.3\%$ in the ASD group and $\frac{9}{78} \approx 11.5\%$ in the TD group. However, a two-proportion test found no statistically significant difference in this frequency of outlier subjects between ASD and TD (p = 0.2668), suggesting that extreme values are concentrated in a small number of individuals rather than representing a systematic group difference. These findings support the validity of the original trimming approach. The extreme values appear to be leverage points or noise rather than having significant impact, and the $\pm 3$ SD trimming in the initial dataset is appropriate. Thus, we can proceed with the original trimmed dataset for our analysis.

### Methodological variations

**Task 3:**

The original analysis selected 10 proteins. To improve the predictive ability, we extended this to 20 proteins. The top 20 proteins from the *t*-tests and random forest were used to fit a logistic regression model. This improved the sensitivity by 6%, specificity by 13%, accuracy by 9%, and AUC by 4%, likely because adding more predictors will explain more of the variability.

Additionally, instead of using a hard intersection between the top 10 proteins, we tested using a fuzzy intersection, meaning we allowed some overlap between proteins from both analyses. The new analysis decreased the sensitivity by 20%, specificity by 7%, accuracy by 10%, and AUC by 12%. This could be because proteins that were found to be important in both analysis likely had the highest predictive power. When using other proteins that are less explanative, the accuracy decreases significantly.

### Improved classifier

**Task 4:** Our alternate protein panel consisted of 35 proteins versus the 5 used in the `inclass-analysis`. We found that combining prior knowledge of protein importance with regularized classification lead to the best model accuracy. Overall our method of combining multiple t-tests, Random Forest importance, and the guided LASSO approach led to improved results compared to the baseline displaying $8\%$ increase in accuracy, $27\%$ increase in specificity, $\approx 3\%$ decrease in AUC, and an $8\%$ decrease in sensitivity. Even though AUC and sensitivity decreased slightly, the other metrics show an overall improvement in classification accuracy.

We also found that just running LASSO regularization on the whole set of proteins led to similar but slightly decreased classification accuracy compared to the baseline approach. In order to navigate around this, the incorporated prior knowledge of the 2-method statistically validated panel of proteins proved useful.
